{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.4.0"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1988863,"sourceType":"datasetVersion","datasetId":1189136}],"dockerImageVersionId":30749,"isInternetEnabled":true,"language":"r","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"library(tidyverse)\nlibrary(data.table)\nlibrary(caret)\nlibrary(randomForest)\nlibrary(xgboost)\nlibrary(corrplot)\nlibrary(ggplot2)\nlibrary(dplyr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = read.csv('/kaggle/input/ethereum-fraud-detection/transaction_dataset.csv', header = TRUE, row.names = NULL)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"summary_df = data.frame(summary(df))\n\n## First 2 columns also do not add any values - indices only.\n## 7 columns are all either null or 0 - 'ERC20AvgTimeBetweenSentTnx', 'ERC20AvgTimeBetweenRecTnx', 'ERC20AvgTimeBetweenRec2Tnx', 'ERC20AvgTimeBetweenContractTnx', 'ERC20MinValSentContract', 'ERC20MaxValSentContract', 'ERC20AvgValSentContract' \n## Can be removed.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Cleaning","metadata":{}},{"cell_type":"code","source":"df[duplicated(df), ]\n\n# Shows no duplicates","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Addresses = data.frame(table(df$Address))\n\n## 9816 unique addresses in 9841 records. This implies presence of duplicates, but with some other column's value different.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[df$Address=='0x4c7520df888aa4569a37ac7d132f89c65821f0af', ]\n\n## All values are same except in the string variables at the end - 'ERC20.most.sent.token.type', 'ERC20_most_rec_token_type'. These columns have more than 50% rows as null, and the remaining values might not add much value to numerical models. Can be safely removed.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df <- subset(df, select = -c(X, \n                             Index, \n                             ERC20.avg.time.between.sent.tnx, \n                             ERC20.avg.time.between.rec.tnx, \n                             ERC20.avg.time.between.rec.2.tnx, \n                             ERC20.avg.time.between.contract.tnx, \n                             ERC20.min.val.sent.contract, \n                             ERC20.max.val.sent.contract, \n                             ERC20.avg.val.sent.contract, \n                             ERC20.most.sent.token.type,\n                             ERC20_most_rec_token_type))\n\n## 40 variables remaining (including our response variable)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[duplicated(df), ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df <- df %>% distinct()\n\n## 9816 observations remian corresponding to unique addresses","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"missing_vals = data.frame(colSums(is.na(df)))\n\n## Shows missing values in columns related to ERC20 token transactions. Nulls might simply mean that no transactions have happened. Therefore, we can fill the nulls with 0 value. ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df[is.na(df)] <- 0","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"## Next, for a few other variables in the summary, we saw that most of them were 0 with very few non-zero values. This implies their variance will be low and hence can be removed because they will not help in detecting fraud. \n\nvariances = data.frame(sapply(df, var, na.rm = TRUE))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## removing columns with low variance\n\ndf <- subset(df, select = -c(min.value.sent.to.contract, \n                             avg.value.sent.to.contract, \n                             max.val.sent.to.contract, \n                             total.ether.sent.contracts, \n                             ERC20.uniq.sent.addr.1))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Normalizing the data before calculating correlation coefficients\npreProc <- preProcess(df[,-which(names(df) == \"FLAG\")], \n                      method = c(\"center\", \"scale\"))\n\ndf_normalized <- predict(preProc, df[,-which(names(df) == \"FLAG\")])\ndf_normalized <- cbind(FLAG = df$FLAG, df_normalized)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Now that we have all unique records, we can safely remove addresses as they will not be of any use to the numerical models that we build. \n\ndf_normalized <- subset(df_normalized, select = -c(Address))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cor_matrix <- cor(df_normalized)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"threshold <- 0.85  # Set your correlation threshold\n\n# Create a list of pairs of highly correlated features\nhighly_correlated_pairs <- list()\n\n# Loop through the correlation matrix\nfor (i in 1:ncol(cor_matrix)) {\n    for (j in 1:(i)) {\n      # print(i)\n      # print(j)\n      if (abs(cor_matrix[i, j]) > threshold & abs(cor_matrix[i, j]) != 1) {\n          pair <- c(colnames(cor_matrix)[i], \n                   colnames(cor_matrix)[j], \n                   cor_matrix[i, j])\n          highly_correlated_pairs[[length(highly_correlated_pairs)+1]] <- pair\n        }\n    }\n}\n\n# Display the list of highly correlated pairs\nfor (pair in highly_correlated_pairs) {\n    cat(pair[1], \"+\", pair[2], \":\", round(as.numeric(pair[3]), 2), \"\\n\") }\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## removing some of correlated variables and keeping only one from each pair\n## We are doing it manually because our dataset is not that big, and has less than 100 features to select from. It is also good for interpretability of our model.  \n\ndf_normalized <- subset(df_normalized, select = -c(ERC20.max.val.rec, \n                             ERC20.avg.val.rec, \n                             ERC20.min.val.sent, \n                             ERC20.max.val.sent, \n                             ERC20.avg.val.sent,\n                             ERC20.uniq.rec.contract.addr))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"{r}\ncor_matrix_new <- cor(df_normalized[,-1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualizations","metadata":{}},{"cell_type":"code","source":"library(reshape2)\n\nggplot(data = melt(cor_matrix_new), aes(x = Var1, y = Var2, fill = value)) + \n  geom_tile(color = \"white\") +\n  # geom_text(aes(label = ifelse(abs(value) > 0.4, round(value, 2), \" \")), size = 2) +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", \n                       midpoint = 0, limit = c(-1, 1), space = \"Lab\",\n                       name = \"Correlation\") +\n  theme_minimal() + \n  theme(axis.text.x = element_text(angle = 90, vjust = 1, hjust = 1)) +\n  coord_fixed() +\n  ggtitle(\"Correlation Matrix of Numerical Features\") +\n  xlab(\"\") + ylab(\"\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ggplot(df, aes(x = log(ERC20.total.Ether.received), fill = as.factor(FLAG))) +\n  geom_histogram(bins = 50, alpha = 0.6, position = \"identity\") +\n  labs(title = \"Distribution of Total Ether Received in exchange of ERC-20 tokens\", x = \"log(Total Ether Received)\", y = \"Count\") +\n  theme_minimal()\n\nggplot(df, aes(x = log(Time.Diff.between.first.and.last..Mins.), fill = as.factor(FLAG))) +\n  geom_histogram(bins = 50, alpha = 0.6, position = \"identity\") +\n  labs(title = \"Distribution of time difference between first and last transaction\", x = \"log(Time Difference)\", y = \"Count\") +\n  theme_minimal()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Anlaysing the flag variable which tells fraud or not\n\nprop.table(table(df_normalized$FLAG))*100\n\nggplot(df_normalized, aes(x = factor(FLAG))) + \n  geom_bar(fill = c(\"lightgreen\", \"pink\")) +\n  ggtitle(\"Distribution of Fraud vs Non-Fraud\") +\n  xlab(\"FRAUD\") +\n  theme_minimal() +\n  geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Calculate counts for each class\ncount_data <- df_normalized %>%\n  group_by(FLAG) %>%\n  summarise(count = n()) %>%\n  mutate(percentage = count/sum(count)*100,\n         ypos = cumsum(percentage) - 0.5*percentage)\n\n# Create donut chart\nggplot(count_data, aes(x = 2, y = percentage, fill = factor(FLAG))) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(theta = \"y\", start = 0) +\n  xlim(0.5, 2.5) +  # Creates the \"hole\" in the middle\n  scale_fill_manual(values = c(\"lightgreen\", \"pink\"), \n                    labels = c(\"Legitimate\", \"Fraudulent\")) +\n  ggtitle(\"Distribution of Fraudulent vs Legitimate\") +\n  theme_minimal() +\n  theme(axis.text = element_blank(),\n        axis.title = element_blank(),\n        panel.grid = element_blank(),\n        legend.position = \"right\") +\n  geom_text(aes(label = paste0(round(percentage,1), \"%\")), \n            position = position_stack(vjust = 0.5), \n            size = 4) +\n  guides(fill = guide_legend(title = \"Labels\"))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The above figure shows mild imbalance in data (3.5 : 1). Since the data is for fraud detection, it might need to be oversampled or downsampled for better training of less robust classification models like logistic regression. For robust algorithms like Random Forest and XGBoost, the imbalance is not huge and no action will be required.\n\n1. Oversampling is good for small datasets as it retains original data and randomly creates new samples.\n\n2. SMOTE is another type of oversampling method. It generates samples for minority class instead of generating random samples. It is also not ideal for a high dimensional dataset but still is the best method for imbalance datasets.\n\n3. For large datasets, it can increase training time and also cause overfitting. As our minority class is not that small, we can go ahead with downsampling of data instead of oversampling. It reduces computational costs while avoiding overfitting to some extent.\n\nWe can try a hybrid approach - SMOTE + downsampling (if the training set becomes very large).\n\nWe will first try training the models without changing the sample ratio and evaluate how the model performs.","metadata":{}},{"cell_type":"code","source":"df_normalized$FLAG = as.factor(df_normalized$FLAG)\n\nset.seed(7406)  # For reproducibility\ntrain_indices <- sample(1:nrow(df_normalized), size = 0.8 * nrow(df_normalized))\ndf_train <- df_normalized[train_indices, ]\ndf_test <- df_normalized[-train_indices, ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"log_reg_model <- glm(FLAG ~ Avg.min.between.sent.tnx + Avg.min.between.received.tnx + Time.Diff.between.first.and.last..Mins. + min.val.sent + avg.val.sent + total.transactions..including.tnx.to.create.contract + ERC20.total.ether.sent + ERC20.uniq.sent.token.name, data = df_train, family = \"binomial\")\n# summary(log_reg_model)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"log_reg_pred_imbal <- predict(log_reg_model, df_test, type = \"response\")\nlog_reg_pred_class_imbal <- as.factor(ifelse(log_reg_pred_imbal >= 0.3, 1, 0))\n\n# Evaluate performance\nconfusionMatrix(log_reg_pred_class_imbal, df_test$FLAG)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Balancing data\n\nSince the models are not performing well on imbalanced data, we will use SMOTE to balance the data first and then build models again.","metadata":{}},{"cell_type":"code","source":"library(smotefamily)\n\ndf_balanced <- SMOTE(X = df_normalized[,-which(names(df_normalized) == \"FLAG\")], \n                   target = df_normalized$FLAG)$data\n\nnames(df_balanced)[names(df_balanced) == \"class\"] <- \"FLAG\"\ndf_balanced$FLAG = as.factor(df_balanced$FLAG)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_downsampled <- df_balanced %>%\n  group_by(FLAG) %>%\n  sample_n(size = min(table(df_balanced$FLAG))) %>%\n  ungroup()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_before <- data.frame(\n  Class = c(\"Legitimate\", \"Fraudulent\"),\n  Count = table(df_normalized$FLAG) %>% as.numeric(),\n  Dataset = \"Before SMOTE\"\n)\n\ndf_after <- data.frame(\n  Class = c(\"Legitimate\", \"Fraudulent\"),\n  Count = table(df_balanced$FLAG) %>% as.numeric(),\n  Dataset = \"After SMOTE\"\n)\n\ndf_after_downsampling = data.frame(\n  Class = c(\"Legitimate\", \"Fraudulent\"),\n  Count = table(df_downsampled$FLAG) %>% as.numeric(),\n  Dataset = \"After Downsampling\"\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"library(gridExtra)\nlibrary(patchwork)\n\ncreate_pie <- function(data, title) {\n  ggplot(data, aes(x = 2, y = Count, fill = Class)) +\n    geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n    coord_polar(theta = \"y\", start = 0) +\n    xlim(0.5, 2.5) +  # Donut hole\n    scale_fill_manual(values = c(\"pink\", \"lightgreen\")) +\n    labs(title = title) +\n    theme_void() +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 12),\n      legend.position = \"right\"\n    ) +\n    geom_text(\n      aes(label = paste0(round(Count / sum(Count) * 100, 1), \"%\")),\n      position = position_stack(vjust = 0.5),\n      size = 4\n    )\n}\n\n# Generate plots\nbefore_plot <- create_pie(df_before, \"Before SMOTE\")\nafter_plot <- create_pie(df_after, \"After SMOTE\")\nafter_downsampling <- create_pie(df_after_downsampling, \"After Downsampling\")\n\n# Align plots with equal size (using patchwork)\ncombined_plots <- before_plot + after_plot + after_downsampling +\n  plot_layout(guides = \"collect\")  # Shared legend\n\n# Display\ncombined_plots\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data is now ready for modeling.","metadata":{}},{"cell_type":"code","source":"set.seed(7406)  # For reproducibility\ntrain_indices <- sample(1:nrow(df_balanced), size = 0.8 * nrow(df_balanced))\ndf_bal_train <- df_balanced[train_indices, ]\ndf_bal_test <- df_balanced[-train_indices, ]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Logistic Regression Model","metadata":{}},{"cell_type":"code","source":"log_reg_model_2 <- glm(FLAG ~ Avg.min.between.sent.tnx+\n                             Avg.min.between.received.tnx+\n                             Time.Diff.between.first.and.last..Mins.+\n                             Sent.tnx+\n                             Unique.Received.From.Addresses+\n                             min.value.received+\n                             avg.val.received+\n                             min.val.sent+\n                             avg.val.sent+\n                             total.transactions..including.tnx.to.create.contract+\n                             total.ether.balance+\n                             Total.ERC20.tnxs+\n                             ERC20.total.ether.sent+\n                             ERC20.total.Ether.sent.contract+\n                             ERC20.uniq.sent.addr+\n                             ERC20.uniq.rec.addr+\n                             ERC20.uniq.sent.token.name+\n                             ERC20.uniq.rec.token.name, \n                       data = df_bal_train, family = \"binomial\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"log_reg_pred <- predict(log_reg_model_2, df_bal_test, type = \"response\")\nlog_reg_pred_class <- as.factor(ifelse(log_reg_pred >= 0.3, 1, 0))\n\n# Evaluate performance\nconfusionMatrix(log_reg_pred_class, as.factor(df_bal_test$FLAG))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Random Forest Model","metadata":{}},{"cell_type":"code","source":"trainControl <- trainControl(method = \"cv\", number = 5)  # 5-fold cross-validation\n\n# Train the Random Forest model with automatic tuning\nrf_model_2 <- train(FLAG ~ ., data = df_bal_train,\n                  method = \"rf\",\n                  trControl = trainControl,\n                  tuneLength = 10)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"varImpPlot(rf_model_2$finalModel)\nplot(rf_model_2$finalModel)\n#print(rf_model_2$finalModel)\n#print(rf_model_2)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_model_final <- randomForest(FLAG ~ ., data = df_bal_train, \n                                ntree = 200, nodesize = 3, mtry=9)\nprint(rf_model_final)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_pred <- predict(rf_model_final, newdata = df_bal_test)\nconfusionMatrix(rf_pred, df_bal_test$FLAG)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"varImpPlot(rf_model_final, main=\"Variable Importance in Random Forest Model\")\nplot(rf_model_final)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Gradient Boosting Model","metadata":{}},{"cell_type":"code","source":"trainControl <- trainControl(method = \"cv\", number = 10) \n# 10-fold cross-validation\n\n# Train the Boosting model (Gradient Boosting)\nboosting_model <- train(FLAG ~ ., data = df_bal_train,\n                        method = \"gbm\",\n                        trControl = trainControl,\n                        tuneLength = 10,  # Try 10 different combinations of parameters\n                        verbose = FALSE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"library(gbm)\ndf_bal_train$FLAG = as.numeric(df_bal_train$FLAG)-1\n\nboosting_model_final = gbm(FLAG ~ ., data = df_bal_train, distribution = \"bernoulli\",\n                   n.trees = 400, interaction.depth = 10, \n                   shrinkage = 0.1, verbose = FALSE)\n\nprint(boosting_model_final)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"boosting_pred <- predict(boosting_model_final, newdata = df_bal_test)\nboosting_pred_class <- as.factor(ifelse(boosting_pred >= 0.3, 1, 0))\n\nconfusionMatrix(boosting_pred_class, df_bal_test$FLAG)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"boosting_importance <- summary(boosting_model, plot = FALSE)\n\n# Load ggplot2\nlibrary(ggplot2)\n\n# Create a ggplot bar chart\nggplot(boosting_importance, aes(x = reorder(var, rel.inf), y = rel.inf)) +\n  geom_bar(stat = \"identity\", fill = \"steelblue\") +\n  coord_flip() +  # Flip for better readability\n  labs(title = \"Variable Importance in Boosting Model\",\n       x = \"Variables\", y = \"Relative Importance\") +\n  theme_minimal()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Evaluation metrics\n\nWe are using accuracy and F1 scores for model evaluation. To formally compare model performance, we perform 10-fold cross-validation, followed by the application of statistical tests like paired t-tests and Wilcoxon tests on the evaluation metrics across the folds.","metadata":{}},{"cell_type":"code","source":"set.seed(7406)\nfolds <- createFolds(df_balanced$FLAG, k = 10, returnTrain = TRUE)\n\nlog_acc <- rf_acc <- gbm_acc <- numeric(length(folds))\nlog_f1 <- rf_f1 <- gbm_f1 <- numeric(length(folds))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"library(MLmetrics)\n\nfor (i in seq_along(folds)) {\n  train_idx <- folds[[i]]\n  train_data <- df_balanced[train_idx, ]\n  test_data <- df_balanced[-train_idx, ]\n  \n  # Logistic Regression (no hyperparams to tune)\n  log_model <- glm(FLAG ~ ., data = train_data, family = binomial)\n  log_pred <- predict(log_model, test_data, type = \"response\")\n  log_class <- ifelse(log_pred > 0.3, 1, 0)\n  log_acc[i] <- mean(log_class == test_data$FLAG)\n  log_f1[i] <- F1_Score(y_pred = log_class, y_true = as.numeric(as.character(test_data$FLAG)), positive = \"1\")\n\n  \n  # Random Forest with chosen hyperparams\n  rf_model <- randomForest(FLAG ~ ., data = train_data, ntree = 200, \n                           mtry = 9, nodesize = 3)\n  rf_pred <- predict(rf_model, test_data)\n  rf_acc[i] <- mean(rf_pred == test_data$FLAG)\n  rf_f1[i] <- F1_Score(y_pred = rf_pred, y_true = test_data$FLAG, positive = \"1\")\n  \n  # GBM with chosen hyperparams\n  train_data$FLAG = as.numeric(train_data$FLAG)-1\n  gbm_model <- gbm(FLAG ~ ., data = train_data, distribution = \"bernoulli\",\n                   n.trees = 400, interaction.depth = 10, \n                   shrinkage = 0.1, verbose = FALSE)\n  gbm_pred <- predict(gbm_model, test_data, type = \"response\")\n  gbm_class <- ifelse(gbm_pred > 0.3, 1, 0)\n  gbm_acc[i] <- mean(gbm_class == test_data$FLAG)\n  gbm_f1[i] <- F1_Score(y_pred = gbm_class, y_true = as.numeric(as.character(test_data$FLAG)), positive = \"1\")\n\n}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"t.test(log_acc, rf_acc, paired = TRUE)\nt.test(log_acc, gbm_acc, paired = TRUE)\nt.test(rf_acc, gbm_acc, paired = TRUE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"t.test(log_f1, rf_f1, paired = TRUE)\nt.test(log_f1, gbm_f1, paired = TRUE)\nt.test(rf_f1, gbm_f1, paired = TRUE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wilcox.test(log_acc, rf_acc, paired = TRUE)\nwilcox.test(log_acc, gbm_acc, paired = TRUE)\nwilcox.test(rf_acc, gbm_acc, paired = TRUE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"wilcox.test(log_f1, rf_f1, paired = TRUE)\nwilcox.test(log_f1, gbm_f1, paired = TRUE)\nwilcox.test(rf_f1, gbm_f1, paired = TRUE)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Some more visualizations","metadata":{}},{"cell_type":"code","source":"g1 = ggplot(as.data.frame(confusionMatrix(log_reg_pred_class_imbal, df_test$FLAG)$table) %>%\n  mutate(Percent = Freq / sum(Freq) * 100,\n         Label = paste0(round(Percent, 1), \"%\")), aes(x = Prediction, y = Reference)) +\n  geom_tile(aes(fill = Percent), color = \"white\") +\n  geom_text(aes(label = Label), vjust = 0.5, fontface = \"bold\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"steelblue\") +\n  labs(title = \"Logistic Regression (imbalanced data)\") +\n  xlab(\"Predicted Class\") +\n  scale_x_discrete(labels = c(\"0\" = \"Legit\", \"1\" = \"Fraud\")) +\n  ylab(\"Actual Class\") +\n  scale_y_discrete(labels = c(\"0\" = \"Legit\", \"1\" = \"Fraud\")) +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\ng2 = ggplot(as.data.frame(confusionMatrix(log_reg_pred_class, df_bal_test$FLAG)$table) %>%\n  mutate(Percent = Freq / sum(Freq) * 100,\n         Label = paste0(round(Percent, 1), \"%\")), aes(x = Prediction, y = Reference)) +\n  geom_tile(aes(fill = Percent), color = \"white\") +\n  geom_text(aes(label = Label), vjust = 0.5, fontface = \"bold\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"steelblue\") +\n  labs(title = \"Logistic Regression (balanced data)\") +\n  xlab(\"Predicted Class\") +\n  scale_x_discrete(labels = c(\"0\" = \"Legit\", \"1\" = \"Fraud\")) +\n  ylab(\"Actual Class\") +\n  scale_y_discrete(labels = c(\"0\" = \"Legit\", \"1\" = \"Fraud\")) +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\ng3 = ggplot(as.data.frame(confusionMatrix(rf_pred, df_bal_test$FLAG)$table) %>%\n  mutate(Percent = Freq / sum(Freq) * 100,\n         Label = paste0(round(Percent, 1), \"%\")), aes(x = Prediction, y = Reference)) +\n  geom_tile(aes(fill = Percent), color = \"white\") +\n  geom_text(aes(label = Label), vjust = 0.5, fontface = \"bold\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"steelblue\") +\n  labs(title = \"Random Forest\") +\n  xlab(\"Predicted Class\") +\n  scale_x_discrete(labels = c(\"0\" = \"Legit\", \"1\" = \"Fraud\")) +\n  ylab(\"Actual Class\") +\n  scale_y_discrete(labels = c(\"0\" = \"Legit\", \"1\" = \"Fraud\")) +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\ng4 = ggplot(as.data.frame(confusionMatrix(boosting_pred, df_bal_test$FLAG)$table) %>%\n  mutate(Percent = Freq / sum(Freq) * 100,\n         Label = paste0(round(Percent, 1), \"%\")), aes(x = Prediction, y = Reference)) +\n  geom_tile(aes(fill = Percent), color = \"white\") +\n  geom_text(aes(label = Label), vjust = 0.5, fontface = \"bold\") +\n  scale_fill_gradient(low = \"lightblue\", high = \"steelblue\") +\n  labs(title = \"Gradient Boosting\") +\n  xlab(\"Predicted Class\") +\n  scale_x_discrete(labels = c(\"0\" = \"Legit\", \"1\" = \"Fraud\")) +\n  ylab(\"Actual Class\") +\n  scale_y_discrete(labels = c(\"0\" = \"Legit\", \"1\" = \"Fraud\")) +\n  theme_minimal(base_size = 14) +\n  theme(legend.position = \"none\")\n\n#grid.arrange(g1, g2, g3, g4, ncol = 2)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}